{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Details project at https://github.com/holedev/improve-cifar10"
      ],
      "metadata": {
        "id": "B6pSGTPTMEBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "TMxZi-wNMVxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UX-5vEnilAI2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def get_default_device():\n",
        "    return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "a09jF3D-MZP9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets"
      ],
      "metadata": {
        "id": "tkv7oY0Tl_yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "03xf7H7MNFql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a66925-777b-4c99-c8fe-c54ae8446702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:11<00:00, 14.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "O6e1etOAmJZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HeheNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4hlXfdGvMhX-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "net = HeheNet().to(device)"
      ],
      "metadata": {
        "id": "158X66U4M6Bf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimize"
      ],
      "metadata": {
        "id": "x8T6f4_vmOvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "9E2Wm2O_NTss"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Lr9dDScemT6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    print(f\"ðŸ“˜ Epoch [{epoch + 1:3d}/{num_epochs}] - Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j999qZ2PN9Su",
        "outputId": "6f70d720-325f-419e-91b3-7e83bc261c6a",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch [  1/100] - Loss: 1.5370\n",
            "ðŸ“˜ Epoch [  2/100] - Loss: 1.1897\n",
            "ðŸ“˜ Epoch [  3/100] - Loss: 1.0365\n",
            "ðŸ“˜ Epoch [  4/100] - Loss: 0.9432\n",
            "ðŸ“˜ Epoch [  5/100] - Loss: 0.8762\n",
            "ðŸ“˜ Epoch [  6/100] - Loss: 0.8163\n",
            "ðŸ“˜ Epoch [  7/100] - Loss: 0.7746\n",
            "ðŸ“˜ Epoch [  8/100] - Loss: 0.7443\n",
            "ðŸ“˜ Epoch [  9/100] - Loss: 0.7086\n",
            "ðŸ“˜ Epoch [ 10/100] - Loss: 0.6815\n",
            "ðŸ“˜ Epoch [ 11/100] - Loss: 0.6571\n",
            "ðŸ“˜ Epoch [ 12/100] - Loss: 0.6392\n",
            "ðŸ“˜ Epoch [ 13/100] - Loss: 0.6145\n",
            "ðŸ“˜ Epoch [ 14/100] - Loss: 0.5972\n",
            "ðŸ“˜ Epoch [ 15/100] - Loss: 0.5802\n",
            "ðŸ“˜ Epoch [ 16/100] - Loss: 0.5650\n",
            "ðŸ“˜ Epoch [ 17/100] - Loss: 0.5485\n",
            "ðŸ“˜ Epoch [ 18/100] - Loss: 0.5330\n",
            "ðŸ“˜ Epoch [ 19/100] - Loss: 0.5240\n",
            "ðŸ“˜ Epoch [ 20/100] - Loss: 0.5078\n",
            "ðŸ“˜ Epoch [ 21/100] - Loss: 0.5000\n",
            "ðŸ“˜ Epoch [ 22/100] - Loss: 0.4828\n",
            "ðŸ“˜ Epoch [ 23/100] - Loss: 0.4695\n",
            "ðŸ“˜ Epoch [ 24/100] - Loss: 0.4605\n",
            "ðŸ“˜ Epoch [ 25/100] - Loss: 0.4513\n",
            "ðŸ“˜ Epoch [ 26/100] - Loss: 0.4430\n",
            "ðŸ“˜ Epoch [ 27/100] - Loss: 0.4286\n",
            "ðŸ“˜ Epoch [ 28/100] - Loss: 0.4271\n",
            "ðŸ“˜ Epoch [ 29/100] - Loss: 0.4154\n",
            "ðŸ“˜ Epoch [ 30/100] - Loss: 0.4071\n",
            "ðŸ“˜ Epoch [ 31/100] - Loss: 0.3933\n",
            "ðŸ“˜ Epoch [ 32/100] - Loss: 0.3845\n",
            "ðŸ“˜ Epoch [ 33/100] - Loss: 0.3847\n",
            "ðŸ“˜ Epoch [ 34/100] - Loss: 0.3758\n",
            "ðŸ“˜ Epoch [ 35/100] - Loss: 0.3648\n",
            "ðŸ“˜ Epoch [ 36/100] - Loss: 0.3616\n",
            "ðŸ“˜ Epoch [ 37/100] - Loss: 0.3535\n",
            "ðŸ“˜ Epoch [ 38/100] - Loss: 0.3465\n",
            "ðŸ“˜ Epoch [ 39/100] - Loss: 0.3376\n",
            "ðŸ“˜ Epoch [ 40/100] - Loss: 0.3352\n",
            "ðŸ“˜ Epoch [ 41/100] - Loss: 0.3318\n",
            "ðŸ“˜ Epoch [ 42/100] - Loss: 0.3239\n",
            "ðŸ“˜ Epoch [ 43/100] - Loss: 0.3129\n",
            "ðŸ“˜ Epoch [ 44/100] - Loss: 0.3131\n",
            "ðŸ“˜ Epoch [ 45/100] - Loss: 0.3068\n",
            "ðŸ“˜ Epoch [ 46/100] - Loss: 0.3006\n",
            "ðŸ“˜ Epoch [ 47/100] - Loss: 0.3016\n",
            "ðŸ“˜ Epoch [ 48/100] - Loss: 0.2894\n",
            "ðŸ“˜ Epoch [ 49/100] - Loss: 0.2878\n",
            "ðŸ“˜ Epoch [ 50/100] - Loss: 0.2821\n",
            "ðŸ“˜ Epoch [ 51/100] - Loss: 0.2775\n",
            "ðŸ“˜ Epoch [ 52/100] - Loss: 0.2723\n",
            "ðŸ“˜ Epoch [ 53/100] - Loss: 0.2663\n",
            "ðŸ“˜ Epoch [ 54/100] - Loss: 0.2670\n",
            "ðŸ“˜ Epoch [ 55/100] - Loss: 0.2600\n",
            "ðŸ“˜ Epoch [ 56/100] - Loss: 0.2549\n",
            "ðŸ“˜ Epoch [ 57/100] - Loss: 0.2492\n",
            "ðŸ“˜ Epoch [ 58/100] - Loss: 0.2466\n",
            "ðŸ“˜ Epoch [ 59/100] - Loss: 0.2409\n",
            "ðŸ“˜ Epoch [ 60/100] - Loss: 0.2414\n",
            "ðŸ“˜ Epoch [ 61/100] - Loss: 0.2413\n",
            "ðŸ“˜ Epoch [ 62/100] - Loss: 0.2339\n",
            "ðŸ“˜ Epoch [ 63/100] - Loss: 0.2283\n",
            "ðŸ“˜ Epoch [ 64/100] - Loss: 0.2212\n",
            "ðŸ“˜ Epoch [ 65/100] - Loss: 0.2197\n",
            "ðŸ“˜ Epoch [ 66/100] - Loss: 0.2157\n",
            "ðŸ“˜ Epoch [ 67/100] - Loss: 0.2141\n",
            "ðŸ“˜ Epoch [ 68/100] - Loss: 0.2169\n",
            "ðŸ“˜ Epoch [ 69/100] - Loss: 0.2062\n",
            "ðŸ“˜ Epoch [ 70/100] - Loss: 0.2069\n",
            "ðŸ“˜ Epoch [ 71/100] - Loss: 0.2011\n",
            "ðŸ“˜ Epoch [ 72/100] - Loss: 0.1993\n",
            "ðŸ“˜ Epoch [ 73/100] - Loss: 0.1959\n",
            "ðŸ“˜ Epoch [ 74/100] - Loss: 0.1952\n",
            "ðŸ“˜ Epoch [ 75/100] - Loss: 0.1866\n",
            "ðŸ“˜ Epoch [ 76/100] - Loss: 0.1884\n",
            "ðŸ“˜ Epoch [ 77/100] - Loss: 0.1847\n",
            "ðŸ“˜ Epoch [ 78/100] - Loss: 0.1807\n",
            "ðŸ“˜ Epoch [ 79/100] - Loss: 0.1787\n",
            "ðŸ“˜ Epoch [ 80/100] - Loss: 0.1778\n",
            "ðŸ“˜ Epoch [ 81/100] - Loss: 0.1798\n",
            "ðŸ“˜ Epoch [ 82/100] - Loss: 0.1737\n",
            "ðŸ“˜ Epoch [ 83/100] - Loss: 0.1689\n",
            "ðŸ“˜ Epoch [ 84/100] - Loss: 0.1730\n",
            "ðŸ“˜ Epoch [ 85/100] - Loss: 0.1628\n",
            "ðŸ“˜ Epoch [ 86/100] - Loss: 0.1632\n",
            "ðŸ“˜ Epoch [ 87/100] - Loss: 0.1616\n",
            "ðŸ“˜ Epoch [ 88/100] - Loss: 0.1631\n",
            "ðŸ“˜ Epoch [ 89/100] - Loss: 0.1575\n",
            "ðŸ“˜ Epoch [ 90/100] - Loss: 0.1569\n",
            "ðŸ“˜ Epoch [ 91/100] - Loss: 0.1549\n",
            "ðŸ“˜ Epoch [ 92/100] - Loss: 0.1529\n",
            "ðŸ“˜ Epoch [ 93/100] - Loss: 0.1526\n",
            "ðŸ“˜ Epoch [ 94/100] - Loss: 0.1519\n",
            "ðŸ“˜ Epoch [ 95/100] - Loss: 0.1481\n",
            "ðŸ“˜ Epoch [ 96/100] - Loss: 0.1466\n",
            "ðŸ“˜ Epoch [ 97/100] - Loss: 0.1434\n",
            "ðŸ“˜ Epoch [ 98/100] - Loss: 0.1430\n",
            "ðŸ“˜ Epoch [ 99/100] - Loss: 0.1379\n",
            "ðŸ“˜ Epoch [100/100] - Loss: 0.1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b7wQv46NmW5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "7lxFUGaimXsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "\n",
        "for classname in classes:\n",
        "    accuracy = 100 * correct_pred[classname] / total_pred[classname]\n",
        "    print(f'=== Accuracy for class: {classname:5s} is {accuracy:.1f} % ===')\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f'ðŸŽ¯ Overall accuracy on 10000 test images: {100 * correct / total:.2f} %')\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "3ZANMNzGOiEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3d542414-317c-49e3-cf93-b60a9e614a4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Accuracy for class: plane is 92.6 % ===\n",
            "=== Accuracy for class: car   is 96.2 % ===\n",
            "=== Accuracy for class: bird  is 83.9 % ===\n",
            "=== Accuracy for class: cat   is 83.4 % ===\n",
            "=== Accuracy for class: deer  is 89.2 % ===\n",
            "=== Accuracy for class: dog   is 84.7 % ===\n",
            "=== Accuracy for class: frog  is 93.1 % ===\n",
            "=== Accuracy for class: horse is 92.8 % ===\n",
            "=== Accuracy for class: ship  is 92.8 % ===\n",
            "=== Accuracy for class: truck is 92.5 % ===\n",
            "==================================================\n",
            "ðŸŽ¯ Overall accuracy on 10000 test images: 90.12 %\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}